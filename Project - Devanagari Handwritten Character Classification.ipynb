{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ProjectPR - Devanagari Handwritten Character Classification.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfBAoLa9yDnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "862oztcszo0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"/content/drive/My Drive/Colab Notebooks/DevanagariHandwrittenCharacterDataset.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM34BrxLvdRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "import shutil\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVyu3Epv_q7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract names of classes from dataset folders\n",
        "def fetchName(folderName):\n",
        "    if folderName[0] == 'c':\n",
        "        return str(folderName[(folderName.index('_',folderName.index('_')+1))+1:])\n",
        "    else:\n",
        "        return str(\"d\"+folderName[folderName.index('_')+1:])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQ2PVTseZOrA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function to perform fetch the entire dataset\n",
        "wholeDataset = []\n",
        "def fetchDataset(rootPath):\n",
        "    folders = os.listdir(rootPath)\n",
        "    for fold in folders:\n",
        "        files = os.listdir(rootPath+fold)\n",
        "        for eachFile in files:\n",
        "            openImage = cv2.imread(rootPath+fold+\"/\"+eachFile)\n",
        "            grayscale = cv2.cvtColor(openImage, cv2.COLOR_BGR2GRAY)\n",
        "            ret,thresh1 = cv2.threshold(grayscale,100,255,cv2.THRESH_BINARY)\n",
        "            singleLineImage = np.array(thresh1).flatten()\n",
        "            replacedValues = [1 if singleLineImage[i]==255 else singleLineImage[i] for i in range(len(singleLineImage))]\n",
        "            wholeDataset.append([replacedValues,fetchName(fold)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wU-T-82EakHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function defined in such a way that each fold can be performed seperately\n",
        "startPoint = int(len(wholeDataset)/5)\n",
        "def crossValidation(k,currentFold):\n",
        "    for i in range(k):\n",
        "        if i == (currentFold-1):\n",
        "            print(\"(\"+str(startPoint*i)+\",\"+str(startPoint*(i+1)-1)+\")\")\n",
        "            performingFold([startPoint*i,startPoint*(i+1)-1],i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnF2MpmvelwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating train and test Arrays and performing SVM \n",
        "def performingFold(startEndPoints,k):\n",
        "    trainDataset = []\n",
        "    testDataset = []\n",
        "    for j in range(len(wholeDataset)):\n",
        "        if j >= startEndPoints[0] and j <= startEndPoints[1]:\n",
        "            testDataset.append(wholeDataset[j])\n",
        "        else:\n",
        "            trainDataset.append(wholeDataset[j])\n",
        "    performSVM(trainDataset, testDataset, k)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCDZamz9f_UE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Collecting the Dataset into a Single Array\n",
        "fetchDataset('DevanagariHandwrittenCharacterDataset/')\n",
        "#Shuffling the Dataset\n",
        "random.shuffle(wholeDataset)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iroKcdb4wGSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accuracyTrace = []\n",
        "def performSVM(trainDataset, testDataset, k):\n",
        "    print(\"Performing Fold-{}\".format(k+1))\n",
        "    print(\"------------------\")\n",
        "    x_train,y_train,x_test,y_test = [],[],[],[]\n",
        "    for p in range(len(trainDataset)):\n",
        "        x_train.append(trainDataset[p][0])\n",
        "        y_train.append(trainDataset[p][1])\n",
        "    for q in range(len(testDataset)):\n",
        "        x_test.append(testDataset[q][0])\n",
        "        y_test.append(testDataset[q][1])\n",
        "    print(np.shape(x_train),np.shape(y_train),np.shape(x_test),np.shape(y_test))\n",
        "\n",
        "    #Building a Support Vector Machine having Gaussian Kernel \n",
        "    model = SVC(kernel='rbf',decision_function_shape='ovo',gamma='scale')\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    #Predicting\n",
        "    y_pred = model.predict(x_test)\n",
        "\n",
        "    #Showing Entire Fold Accuracy as well as Class Wise\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    #Storing Accuracy\n",
        "    accuracyTrace.append(accuracy_score(y_test,y_pred))\n",
        "    print(\"Fold Accuracy: {}\".format(accuracy_score(y_test,y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cuOqA2NhBsu",
        "colab_type": "code",
        "outputId": "53820dfc-d934-4ce3-d968-f2f3d647b798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#5-Fold Cross Validation\n",
        "#Fold No. 1\n",
        "crossValidation(5,1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0,18399)\n",
            "Performing Fold-1\n",
            "------------------\n",
            "(73600, 1024) (73600,) (18400, 1024) (18400,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        adna       0.89      0.96      0.92       368\n",
            "          ba       0.89      0.84      0.87       404\n",
            "         bha       0.91      0.91      0.91       405\n",
            "         cha       0.91      0.95      0.93       454\n",
            "        chha       0.91      0.91      0.91       383\n",
            "       chhya       0.91      0.92      0.91       400\n",
            "          d0       0.98      0.99      0.99       385\n",
            "          d1       0.97      0.99      0.98       415\n",
            "          d2       0.94      0.96      0.95       408\n",
            "          d3       0.96      0.95      0.95       403\n",
            "          d4       0.97      0.98      0.97       383\n",
            "          d5       0.97      0.99      0.98       409\n",
            "          d6       0.98      0.97      0.97       461\n",
            "          d7       0.99      0.98      0.98       374\n",
            "          d8       0.97      0.99      0.98       403\n",
            "          d9       0.98      0.98      0.98       407\n",
            "          da       0.90      0.88      0.89       394\n",
            "         daa       0.92      0.92      0.92       368\n",
            "         dha       0.90      0.90      0.90       397\n",
            "        dhaa       0.95      0.93      0.94       374\n",
            "          ga       0.94      0.90      0.92       386\n",
            "         gha       0.86      0.92      0.89       409\n",
            "         gya       0.94      0.94      0.94       415\n",
            "          ha       0.93      0.92      0.93       377\n",
            "          ja       0.94      0.95      0.95       371\n",
            "         jha       0.98      0.95      0.97       427\n",
            "          ka       0.96      0.96      0.96       385\n",
            "         kha       0.94      0.90      0.92       444\n",
            "         kna       0.94      0.90      0.92       397\n",
            "          la       0.98      0.94      0.96       387\n",
            "          ma       0.90      0.89      0.89       413\n",
            "     motosaw       0.89      0.94      0.92       401\n",
            "          na       0.93      0.91      0.92       439\n",
            "          pa       0.86      0.92      0.89       413\n",
            "   patalosaw       0.87      0.90      0.88       389\n",
            "petchiryakha       0.91      0.91      0.91       384\n",
            "         pha       0.96      0.97      0.96       423\n",
            "          ra       0.96      0.95      0.96       413\n",
            "    taamatar       0.94      0.95      0.94       385\n",
            "      tabala       0.97      0.93      0.95       374\n",
            "         tha       0.89      0.87      0.88       429\n",
            "        thaa       0.95      0.94      0.94       407\n",
            "         tra       0.92      0.90      0.91       378\n",
            "         waw       0.90      0.91      0.90       379\n",
            "         yaw       0.86      0.86      0.86       390\n",
            "         yna       0.96      0.90      0.93       390\n",
            "\n",
            "    accuracy                           0.93     18400\n",
            "   macro avg       0.93      0.93      0.93     18400\n",
            "weighted avg       0.93      0.93      0.93     18400\n",
            "\n",
            "Fold Accuracy: 0.931141304347826\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHh5xahPCXKS",
        "colab_type": "code",
        "outputId": "50dbfb63-44b3-4b51-c942-68c26cf8a335",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Fold No. 2\n",
        "crossValidation(5,2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(18400,36799)\n",
            "Performing Fold-2\n",
            "------------------\n",
            "(73600, 1024) (73600,) (18400, 1024) (18400,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        adna       0.90      0.95      0.93       432\n",
            "          ba       0.88      0.86      0.87       416\n",
            "         bha       0.93      0.90      0.91       413\n",
            "         cha       0.90      0.93      0.92       372\n",
            "        chha       0.93      0.89      0.91       404\n",
            "       chhya       0.92      0.93      0.93       424\n",
            "          d0       0.97      0.99      0.98       378\n",
            "          d1       0.97      0.99      0.98       388\n",
            "          d2       0.94      0.97      0.95       396\n",
            "          d3       0.96      0.96      0.96       422\n",
            "          d4       0.97      0.98      0.98       387\n",
            "          d5       0.97      0.98      0.98       393\n",
            "          d6       0.98      0.98      0.98       383\n",
            "          d7       0.99      0.97      0.98       392\n",
            "          d8       0.97      0.98      0.97       406\n",
            "          d9       0.98      1.00      0.99       412\n",
            "          da       0.86      0.88      0.87       405\n",
            "         daa       0.88      0.89      0.89       390\n",
            "         dha       0.88      0.89      0.89       421\n",
            "        dhaa       0.92      0.92      0.92       398\n",
            "          ga       0.94      0.93      0.94       396\n",
            "         gha       0.85      0.87      0.86       393\n",
            "         gya       0.92      0.94      0.93       385\n",
            "          ha       0.92      0.94      0.93       407\n",
            "          ja       0.94      0.93      0.94       381\n",
            "         jha       0.98      0.95      0.97       374\n",
            "          ka       0.97      0.97      0.97       402\n",
            "         kha       0.91      0.94      0.93       397\n",
            "         kna       0.92      0.86      0.89       370\n",
            "          la       0.95      0.93      0.94       421\n",
            "          ma       0.89      0.86      0.88       383\n",
            "     motosaw       0.90      0.93      0.91       381\n",
            "          na       0.89      0.92      0.91       384\n",
            "          pa       0.84      0.90      0.87       378\n",
            "   patalosaw       0.88      0.87      0.88       416\n",
            "petchiryakha       0.90      0.94      0.92       405\n",
            "         pha       0.97      0.95      0.96       377\n",
            "          ra       0.95      0.94      0.95       394\n",
            "    taamatar       0.97      0.92      0.95       426\n",
            "      tabala       0.94      0.95      0.95       442\n",
            "         tha       0.88      0.84      0.86       409\n",
            "        thaa       0.96      0.94      0.95       417\n",
            "         tra       0.89      0.90      0.89       393\n",
            "         waw       0.91      0.88      0.89       423\n",
            "         yaw       0.89      0.84      0.86       420\n",
            "         yna       0.95      0.93      0.94       394\n",
            "\n",
            "    accuracy                           0.93     18400\n",
            "   macro avg       0.93      0.93      0.93     18400\n",
            "weighted avg       0.93      0.93      0.93     18400\n",
            "\n",
            "Fold Accuracy: 0.9265217391304348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzH4MbVwCpiw",
        "colab_type": "code",
        "outputId": "ec9c8957-3937-4222-be89-c3cfd60ef826",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Fold No. 3\n",
        "crossValidation(5,3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(36800,55199)\n",
            "Performing Fold-3\n",
            "------------------\n",
            "(73600, 1024) (73600,) (18400, 1024) (18400,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        adna       0.91      0.96      0.93       390\n",
            "          ba       0.88      0.85      0.87       406\n",
            "         bha       0.93      0.93      0.93       368\n",
            "         cha       0.92      0.96      0.94       374\n",
            "        chha       0.92      0.88      0.90       392\n",
            "       chhya       0.94      0.91      0.93       438\n",
            "          d0       0.99      1.00      0.99       422\n",
            "          d1       0.97      0.99      0.98       406\n",
            "          d2       0.95      0.95      0.95       400\n",
            "          d3       0.94      0.95      0.95       387\n",
            "          d4       0.96      0.99      0.98       433\n",
            "          d5       0.98      0.99      0.99       387\n",
            "          d6       0.97      0.97      0.97       409\n",
            "          d7       0.98      0.99      0.99       410\n",
            "          d8       0.97      0.97      0.97       390\n",
            "          d9       0.99      0.99      0.99       403\n",
            "          da       0.92      0.89      0.91       404\n",
            "         daa       0.86      0.92      0.89       371\n",
            "         dha       0.90      0.89      0.90       379\n",
            "        dhaa       0.92      0.94      0.93       422\n",
            "          ga       0.95      0.92      0.94       423\n",
            "         gha       0.89      0.90      0.90       409\n",
            "         gya       0.95      0.92      0.93       433\n",
            "          ha       0.94      0.91      0.93       435\n",
            "          ja       0.93      0.95      0.94       408\n",
            "         jha       0.97      0.98      0.98       427\n",
            "          ka       0.96      0.97      0.96       377\n",
            "         kha       0.95      0.89      0.92       403\n",
            "         kna       0.92      0.88      0.90       399\n",
            "          la       0.97      0.94      0.96       391\n",
            "          ma       0.92      0.92      0.92       409\n",
            "     motosaw       0.86      0.93      0.89       352\n",
            "          na       0.93      0.90      0.91       375\n",
            "          pa       0.87      0.91      0.89       433\n",
            "   patalosaw       0.89      0.87      0.88       397\n",
            "petchiryakha       0.88      0.93      0.91       393\n",
            "         pha       0.96      0.96      0.96       410\n",
            "          ra       0.97      0.95      0.96       382\n",
            "    taamatar       0.94      0.96      0.95       400\n",
            "      tabala       0.93      0.95      0.94       413\n",
            "         tha       0.87      0.85      0.86       356\n",
            "        thaa       0.94      0.94      0.94       377\n",
            "         tra       0.90      0.93      0.91       401\n",
            "         waw       0.90      0.87      0.88       391\n",
            "         yaw       0.89      0.88      0.89       403\n",
            "         yna       0.96      0.93      0.95       412\n",
            "\n",
            "    accuracy                           0.93     18400\n",
            "   macro avg       0.93      0.93      0.93     18400\n",
            "weighted avg       0.93      0.93      0.93     18400\n",
            "\n",
            "Fold Accuracy: 0.9323369565217391\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2MY8vYVYCqJs",
        "colab_type": "code",
        "outputId": "714520cb-e8a2-432e-b294-565dad84dc82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Fold No. 4\n",
        "crossValidation(5,4)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(55200,73599)\n",
            "Performing Fold-4\n",
            "------------------\n",
            "(73600, 1024) (73600,) (18400, 1024) (18400,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        adna       0.92      0.94      0.93       422\n",
            "          ba       0.89      0.88      0.88       392\n",
            "         bha       0.91      0.91      0.91       411\n",
            "         cha       0.91      0.94      0.92       409\n",
            "        chha       0.93      0.93      0.93       404\n",
            "       chhya       0.93      0.97      0.95       360\n",
            "          d0       0.99      0.99      0.99       395\n",
            "          d1       0.98      0.99      0.99       406\n",
            "          d2       0.94      0.97      0.95       381\n",
            "          d3       0.96      0.96      0.96       400\n",
            "          d4       0.97      0.98      0.98       425\n",
            "          d5       0.98      0.97      0.98       446\n",
            "          d6       0.99      0.96      0.97       353\n",
            "          d7       0.99      1.00      0.99       421\n",
            "          d8       0.97      0.98      0.98       402\n",
            "          d9       0.99      0.99      0.99       371\n",
            "          da       0.91      0.89      0.90       377\n",
            "         daa       0.90      0.93      0.91       427\n",
            "         dha       0.87      0.87      0.87       389\n",
            "        dhaa       0.95      0.94      0.95       414\n",
            "          ga       0.95      0.92      0.93       428\n",
            "         gha       0.88      0.86      0.87       413\n",
            "         gya       0.94      0.93      0.94       375\n",
            "          ha       0.91      0.94      0.93       378\n",
            "          ja       0.94      0.93      0.94       414\n",
            "         jha       0.97      0.96      0.97       364\n",
            "          ka       0.97      0.97      0.97       408\n",
            "         kha       0.91      0.93      0.92       373\n",
            "         kna       0.94      0.91      0.92       398\n",
            "          la       0.97      0.92      0.94       428\n",
            "          ma       0.89      0.84      0.86       389\n",
            "     motosaw       0.89      0.92      0.90       437\n",
            "          na       0.93      0.91      0.92       413\n",
            "          pa       0.83      0.93      0.87       405\n",
            "   patalosaw       0.88      0.85      0.87       372\n",
            "petchiryakha       0.89      0.91      0.90       406\n",
            "         pha       0.96      0.94      0.95       358\n",
            "          ra       0.97      0.95      0.96       402\n",
            "    taamatar       0.97      0.95      0.96       408\n",
            "      tabala       0.93      0.96      0.95       400\n",
            "         tha       0.89      0.88      0.88       384\n",
            "        thaa       0.96      0.96      0.96       403\n",
            "         tra       0.91      0.90      0.90       415\n",
            "         waw       0.88      0.87      0.88       391\n",
            "         yaw       0.87      0.87      0.87       415\n",
            "         yna       0.95      0.94      0.94       418\n",
            "\n",
            "    accuracy                           0.93     18400\n",
            "   macro avg       0.93      0.93      0.93     18400\n",
            "weighted avg       0.93      0.93      0.93     18400\n",
            "\n",
            "Fold Accuracy: 0.9309782608695653\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4DowLotCq2K",
        "colab_type": "code",
        "outputId": "31650ba8-30c8-4011-f758-3ffb24c0a885",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Fold No. 5\n",
        "crossValidation(5,5)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(73600,91999)\n",
            "Performing Fold-5\n",
            "------------------\n",
            "(73600, 1024) (73600,) (18400, 1024) (18400,)\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        adna       0.90      0.96      0.93       388\n",
            "          ba       0.86      0.86      0.86       382\n",
            "         bha       0.93      0.92      0.92       403\n",
            "         cha       0.92      0.93      0.92       391\n",
            "        chha       0.92      0.88      0.90       417\n",
            "       chhya       0.95      0.94      0.95       378\n",
            "          d0       0.98      1.00      0.99       420\n",
            "          d1       0.96      0.99      0.97       385\n",
            "          d2       0.94      0.98      0.96       415\n",
            "          d3       0.96      0.96      0.96       388\n",
            "          d4       0.97      0.97      0.97       372\n",
            "          d5       0.96      0.98      0.97       365\n",
            "          d6       0.98      0.97      0.97       394\n",
            "          d7       0.98      0.99      0.99       403\n",
            "          d8       0.97      0.98      0.98       399\n",
            "          d9       0.98      0.99      0.98       407\n",
            "          da       0.92      0.92      0.92       420\n",
            "         daa       0.90      0.89      0.90       444\n",
            "         dha       0.89      0.87      0.88       414\n",
            "        dhaa       0.94      0.93      0.93       392\n",
            "          ga       0.93      0.93      0.93       367\n",
            "         gha       0.86      0.86      0.86       376\n",
            "         gya       0.93      0.95      0.94       392\n",
            "          ha       0.92      0.90      0.91       403\n",
            "          ja       0.96      0.94      0.95       426\n",
            "         jha       0.98      0.96      0.97       408\n",
            "          ka       0.98      0.96      0.97       428\n",
            "         kha       0.94      0.91      0.92       383\n",
            "         kna       0.93      0.87      0.90       436\n",
            "          la       0.96      0.95      0.96       373\n",
            "          ma       0.89      0.89      0.89       406\n",
            "     motosaw       0.90      0.94      0.92       429\n",
            "          na       0.91      0.91      0.91       389\n",
            "          pa       0.83      0.91      0.86       371\n",
            "   patalosaw       0.90      0.87      0.88       426\n",
            "petchiryakha       0.88      0.94      0.91       412\n",
            "         pha       0.97      0.95      0.96       432\n",
            "          ra       0.96      0.91      0.94       409\n",
            "    taamatar       0.96      0.97      0.96       381\n",
            "      tabala       0.96      0.95      0.96       371\n",
            "         tha       0.90      0.84      0.87       422\n",
            "        thaa       0.95      0.96      0.95       396\n",
            "         tra       0.91      0.91      0.91       413\n",
            "         waw       0.90      0.89      0.89       416\n",
            "         yaw       0.86      0.90      0.88       372\n",
            "         yna       0.96      0.94      0.95       386\n",
            "\n",
            "    accuracy                           0.93     18400\n",
            "   macro avg       0.93      0.93      0.93     18400\n",
            "weighted avg       0.93      0.93      0.93     18400\n",
            "\n",
            "Fold Accuracy: 0.9302717391304348\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTTQaueGwH2Q",
        "colab_type": "code",
        "outputId": "7d1ca5ea-f2f7-459f-b3b5-ed83c67101e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"Average Accuracy: {}\".format(np.mean(accuracyTrace)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Accuracy: 0.93025\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}